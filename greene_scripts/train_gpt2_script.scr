#!/bin/bash
#SBATCH --job-name=train_gpt2
#SBATCH --open-mode=append
#SBATCH --output=/scratch/ka2773/project/lm-mem/logs/%j_%x.out
#SBATCH --error=/scratch/ka2773/project/lm-mem/logs/%j_%x.err
#SBATCH --export=ALL
#SBATCH --time=12:00:00
#SBATCH --gres=gpu:1
#SBATCH --mem=40G
#SBATCH -c 4
#SBATCH --mail-type=end
#SBATCH --mail-user=karmeni1@jhu.edu

# collect input arguments
login_key=$1
config_file=$2

# source config file which loads variables ($ds_size etc.)
source $config_file

# set wanbd variable names
wandb_group=$ds_size
wandb_name=$n_layer$"-"$n_embed"-"$seq_len
wandb_notes=$3
wandb_project="gpt2"

dataset_dir=$HOME/project/lm-mem/data/wikitext-103
model_name="gpt2_"$ds_size"_"$wandb_name"_a"
model_filename=$model_name".pth"

# set up singularity image
singularity exec --nv --overlay $SCRATCH/overlay-50G-10M.ext3:ro /scratch/work/public/singularity/cuda10.2-cudnn8-devel-ubuntu18.04.sif /bin/bash -c "

# source conda shell comands
source /ext3/env.sh

# activate environment with pytorch1.6
conda activate core_env

# call training script
python $HOME/project/lm-mem/src/train_gpt2_.py --train_ds $dataset_dir/wiki.train.inds_$ds_size.bpe.json \
                                               --val_ds $dataset_dir/wiki.valid.inds.bpe.json \
                                               --test_ds $dataset_dir/wiki.test.inds.bpe.json \
                                               --sequence_len 1024 \
                                               --do_train \
                                               --model_name $model_filename \
                                               --tokenizer_path $HOME/project/lm-mem/data/wikitext-103_tokenizer \
                                               --seed $seed \
					       --device cuda \
					       --train_batch_size $train_batch_size \
					       --eval_batch_size $eval_batch_size \
					       --test_batch_size $test_batch_size \
					       --n_layer $n_layer \
					       --n_head $n_head \
					       --embed_dim $n_embed \
					       --max_epochs $max_epochs \
					       --lr_scheduler_type $lr_scheduler_type \
					       --lr $lr \
					       --adam_beta1 $adam_beta1 \
					       --adam_beta2 $adam_beta2 \
					       --num_lr_warmup_steps 0 \
					       --num_eval_steps 500 \
					       --num_logging_steps 500 \
					       --num_save_steps 500 \
					       --es_patience $es_patience \
					       --test_stride $test_stride \
					       --wandb_key $login_key \
					       --wandb_dir $SCRATCH/project/lm-mem/ \
					       --wandb_project $wandb_project \
					       --wandb_group $wandb_group \
					       --wandb_name $wandb_name \
					       --wandb_notes $wandb_notes \
					       --wandb_mode online \
					       --savedir $SCRATCH/project/lm-mem/checkpoints/$model_name \
					       --logdir $SCRATCH/project/lm-mem/logs/$model_name

"
